{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49d46ac3-3fc6-4790-bd80-c9ca747bc5b0",
   "metadata": {},
   "source": [
    "# Vision Transformers in Action\n",
    "## Building and Training ViT to classify images\n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand how Vision Transformers process images as sequences\n",
    "- Implement ViT from scratch using PyTorch\n",
    "- Train on real datasets from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f24604-98e7-45b3-ad6b-5e0febb77f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    import google.colab\n",
    "    !pip install transformers datasets torch torchvision matplotlib numpy pillow tqdm\n",
    "    ON_COLAB = True\n",
    "except:\n",
    "    print(\"Running Locally\")\n",
    "    ON_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54cd637-6ace-4efc-b63e-9c0e819afa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Set device and seeds for reproducibility\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085f346b-8731-43f5-bd17-91178172e658",
   "metadata": {},
   "source": [
    "## Part 1: Loading and Preparing Data\n",
    "\n",
    "### 1.1 First let's download the `food101' Dataset from Hugging Face\n",
    "\n",
    "If running locally, you can save time downloading the dataset by downloading from Google Drive via [this link](https://drive.google.com/file/d/1W_0BAmoT2gkSKf_vEiEJYUGLX8fKOBK_/view?usp=sharing) with Monash account. Unzip it and paste that to the `datasets` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4197646-958a-4bed-a768-a1f260b6f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"food101\"\n",
    "def prepare_data(dataset_name=dataset_name):\n",
    "    \"\"\"Load dataset from Hugging Face and create data loaders\"\"\"\n",
    "\n",
    "    # Load dataset\n",
    "    train_dataset = load_dataset(dataset_name, split=\"train[:5000]\")\n",
    "    val_dataset = load_dataset(dataset_name, split=\"validation[:500]\")\n",
    "\n",
    "    dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    dataset = dataset.shuffle(seed=42) # Shuffle for randomness\n",
    "    dataset_small = dataset.select(range(5000)) # Select only 5000 samples\n",
    "    splits = dataset_small.train_test_split(test_size=0.1, seed=42)     # Split into 90/10\n",
    "    train_dataset = splits[\"train\"]\n",
    "    val_dataset = splits[\"test\"]\n",
    "\n",
    "    # class names\n",
    "    class_names = dataset_small.features[\"label\"].names\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        # Can add more data augmentation here\n",
    "        # Suggestions: RandomHorizontalFlip, RandomCrop, ColorJitter\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    return train_dataset, val_dataset, train_transform, val_transform, class_names\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Custom dataset wrapper for the dataset\"\"\"\n",
    "\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item['image']\n",
    "        label = item['label']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Load data\n",
    "train_data, test_data, train_transform, val_transform, class_names = prepare_data()\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ImageDataset(train_data, train_transform)\n",
    "test_dataset = ImageDataset(test_data, val_transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, persistent_workers=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, persistent_workers=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7fd241-b7a8-4d8f-886d-ac8127f0afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize for visualization\n",
    "mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "def denormalize(tensor):\n",
    "    return tensor * std + mean\n",
    "\n",
    "def visualize_image_samples(images, labels):\n",
    "    \"\"\"Visualize some samples\"\"\"\n",
    "    # Create grid\n",
    "    grid = make_grid(denormalize(images), nrow=4, padding=2)\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(grid.permute(1, 2, 0).clamp(0, 1))\n",
    "    plt.title(\"Training Samples\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Print labels\n",
    "    print(\"Labels for displayed images:\")\n",
    "    for i in range(16):\n",
    "        print(f\"{class_names[labels[i]]}\", end=\"  \")\n",
    "        if (i + 1) % 4 == 0:\n",
    "            print()\n",
    "\n",
    "    plt.show()\n",
    "# Get a batch of training data\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "visualize_image_samples(images[:16], labels[:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d6e2fa-1d1e-4bd1-9576-81fe1f2571ec",
   "metadata": {},
   "source": [
    "## Part 2: Understanding ViT *Architecture*\n",
    "\n",
    "### 2.1 Image Patches: From Pixels to Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f36477a-a220-40c6-9314-0a9ba78da4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(image, patch_size=16):\n",
    "    \"\"\"Extract non-overlapping patches from image\"\"\"\n",
    "    # image shape: (C, H, W)\n",
    "    C, H, W = image.shape\n",
    "    print(C,H,W)\n",
    "    #TODO 1: Calculate number of patches and patches along the width (w) and heigh (h)\n",
    "    num_patches_h = None # YOUR CODE HERE\n",
    "    num_patches_w = None # YOUR CODE HERE\n",
    "    patches = None # YOUR CODE HERE\n",
    "    #ENDTODO ------------------------------------------------------------\n",
    "\n",
    "    return patches, num_patches_h, num_patches_w\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "image = train_data[0]['image']\n",
    "image_tensor = transform(image)\n",
    "\n",
    "patches, num_h, num_w = extract_patches(image_tensor, patch_size=32)\n",
    "print(len(patches))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Original image\n",
    "axes[0, 0].imshow(image)\n",
    "axes[0, 0].set_title(\"Original Image\")\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Show first few patches\n",
    "for i in range(8):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    patch_idx = i - 1\n",
    "    if patch_idx < len(patches):\n",
    "        patch = patches[patch_idx].permute(1, 2, 0)\n",
    "        row, col = divmod(i, 4)\n",
    "        axes[row, col].imshow(patch)\n",
    "        axes[row, col].set_title(f\"Patch {patch_idx}\")\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image shape: {image_tensor.shape}\")\n",
    "print(f\"Number of patches: {len(patches)}\")\n",
    "print(f\"Patch shape: {patches[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4082b6b5-8eeb-466b-a3e8-0d10f8d45445",
   "metadata": {},
   "source": [
    "### 2.2 The Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6face06-b3d8-4c08-9ac7-409958367776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Convert image patches to embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        #TODO 2: Implement patch embedding using convolution\n",
    "        # Hint: Conv2d with kernel_size=patch_size, stride=patch_size\n",
    "        self.projection = None # YOUR CODE HERE\n",
    "        #ENDTODO ------------------------------------------------------------\n",
    "        \n",
    "\n",
    "        # Learnable position embeddings\n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            torch.randn(1, self.num_patches + 1, embed_dim) * 0.02\n",
    "        )\n",
    "\n",
    "        # Class token (like BERT's [CLS] token)\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, embed_dim) * 0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        #TODO 2: Implement forward pass\n",
    "        # 1. Extract patches using convolution # Output shape: (batch_size, embed_dim, num_patches_h, num_patches_w)\n",
    "        # 2. Flatten spatial dimensions  (height, width) of the patches and transpose to the standard transformer format (batch_size, seq_len, embed_dim). # (B, E, H, W) -> (B, E, N) -> (B, N, E) where N is num_patches\n",
    "        # 3. Add class token. Prepend the class token to the sequence of patches. Expand the class token to match the batch size.\n",
    "        # 4. Add position embeddings. This is broadcasted across the batch dimension.\n",
    "        \n",
    "        # Extract patches\n",
    "        x =  None # YOUR CODE HERE\n",
    "        x =  None # YOUR CODE HERE: flatten and transpose\n",
    "        \n",
    "        # Add class token\n",
    "        class_tokens = None # YOUR CODE HERE\n",
    "        x =  None # YOUR CODE HERE: concatenate\n",
    "        \n",
    "        # Add position embeddings\n",
    "        x =  None # YOUR CODE HERE\n",
    "        #ENDTODO ------------------------------------------------------------\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0893ddd1-63df-4e21-b49d-b274380c0d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ViT Block\n",
    "class ViTBlock(nn.Module):\n",
    "    \"\"\"Single Vision Transformer block\"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Multi-head self-attention\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim, num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "\n",
    "        # MLP (Feed-forward)\n",
    "        mlp_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),  # ViT uses GELU instead of ReLU\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # Layer normalization (pre-norm like in ViT)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #TODO 4: Implement ViT block forward pass with pre-normalization\n",
    "        # Note: ViT uses pre-norm (norm before attention/mlp) unlike original transformer\n",
    "        \n",
    "        # Self-attention with pre-norm\n",
    "        residual = x\n",
    "        x = None # YOUR CODE HERE: apply norm1\n",
    "        x, _ = None # YOUR CODE HERE: apply attention  (query, key, and value are the same)\n",
    "        x = None # YOUR CODE HERE: add residual\n",
    "        \n",
    "        # MLP with pre-norm\n",
    "        residual = x\n",
    "        x = None # YOUR CODE HERE: apply norm2\n",
    "        x = None # YOUR CODE HERE: apply mlp\n",
    "        x = None # YOUR CODE HERE: add residual\n",
    "        #ENDTODO ------------------------------------------------------------\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360adbb-f438-44ac-8eda-9e1108274118",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ON_COLAB:\n",
    "    import otter\n",
    "    from ece4078.Utility import pretty_print_otter\n",
    "    grader = otter.Notebook(tests_dir = \"Practical10_Support/tests\")\n",
    "    grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760bb382-c4b1-41fa-bb2b-05c0015eed22",
   "metadata": {},
   "source": [
    "# [NOT GRADED] Vision Transformer in action\n",
    "\n",
    "## 1. Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fd71bf-673e-4507-ba5b-1f745458c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Complete Vision Transformer implementation\"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, num_classes=1000,\n",
    "                 embed_dim=768, num_layers=12, num_heads=12, mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ViTBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final layer norm\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # Classification head\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights following ViT paper\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.trunc_normal_(module.weight, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "            nn.init.constant_(module.weight, 1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # Pass through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Final norm\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Use class token for classification\n",
    "        class_token = x[:, 0]\n",
    "\n",
    "        # Classification\n",
    "        output = self.head(class_token)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Create model\n",
    "model = VisionTransformer(\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    num_classes=len(class_names),\n",
    "    embed_dim=384,   # Smaller for faster training\n",
    "    num_layers=6,    # Smaller for demo\n",
    "    num_heads=6\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96da0aa-7bb5-49fd-b23d-fcffbdfab60b",
   "metadata": {},
   "source": [
    "## 2. Let's try supervised classification (Standard Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd95bcfe-8a4d-4eca-a705-6418a6421dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised(model, train_loader, val_loader, num_epochs=5):\n",
    "    \"\"\"Standard supervised training for image classification\"\"\"\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Print progress every 100 batches\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        train_losses.append(avg_loss)\n",
    "        val_accuracies.append(accuracy)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    return train_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd6a92-b76c-49e3-8b17-b534f9f4a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE TRAINING \n",
    "print(\"Starting supervised training...\")\n",
    "sup_losses, sup_accuracies = train_supervised(model, train_loader, test_loader, num_epochs=50)\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    # visualize the first 16\n",
    "    visualize_image_samples(images[:16].to('cpu'), labels[:16].to('cpu'))\n",
    "    print([class_names[i] for i in predicted[:16]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
