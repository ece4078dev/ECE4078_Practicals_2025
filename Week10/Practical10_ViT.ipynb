{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2405ab0-f956-4a44-a2bc-3e1217c0d86a",
   "metadata": {},
   "source": [
    "# Transformers for Translation in Action\n",
    "Building and Training Encoder-Decoder Architecture to translate English to German\n",
    "\n",
    "## Learning Objectives\n",
    "* Understand how Transformer models process sequential text data\n",
    "* Implement Encoder-Decoder Transformer from scratch using PyTorch based on \"Attention is All You Need\" by Vaswani et al. (2017)\n",
    "* Train and evaluate on real translation datasets from Hugging Face\n",
    "\n",
    "References:\n",
    "* [Attention is All You Need ](https://arxiv.org/abs/1706.03762)\n",
    "* [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71e67bd-c0b1-4a72-9dfd-5e21c85681a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    import google.colab\n",
    "    !pip install transformers datasets tokenizers torch matplotlib numpy\n",
    "except:\n",
    "    print(\"Running Locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f1b0f-e1a0-49da-9ab7-456619b60f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import math\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set device, Make sure you have a GPU sessions. Runtime -> Change runtime Type -> T4 GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad7dbd-24be-488f-86cb-75fd626683ef",
   "metadata": {},
   "source": [
    "## Part 1: Loading and Preparing Data\n",
    "\n",
    "### 1.1 Load Dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d02807-79ea-46c0-9a47-cad5cbab578f",
   "metadata": {},
   "source": [
    "If running locally, you can save time downloading the dataset by downloading from Google Drive via [this link](https://drive.google.com/file/d/1W_0BAmoT2gkSKf_vEiEJYUGLX8fKOBK_/view?usp=sharing) with Monash account. Unzip it and paste that to the `datasets` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cd958a-14dd-412b-910f-c5ff15f9493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_translation_data():\n",
    "    \"\"\"Load and preprocess WMT14 EN-DE dataset\"\"\"\n",
    "    # Using a smaller subset for demonstration\n",
    "    dataset = load_dataset(\"wmt14\", \"de-en\", split=\"train[:10000]\")  # Small subset\n",
    "    val_dataset = load_dataset(\"wmt14\", \"de-en\", split=\"validation[:1000]\")\n",
    "\n",
    "    # Extract English and German sentences\n",
    "    en_sentences = [item['translation']['en'] for item in dataset]\n",
    "    de_sentences = [item['translation']['de'] for item in dataset]\n",
    "\n",
    "    val_en = [item['translation']['en'] for item in val_dataset]\n",
    "    val_de = [item['translation']['de'] for item in val_dataset]\n",
    "\n",
    "    return en_sentences, de_sentences, val_en, val_de\n",
    "\n",
    "en_train, de_train, en_val, de_val = load_translation_data()\n",
    "print(\"Dataset loaded!\")\n",
    "print(f\"Training examples: {len(en_train)}\")\n",
    "print(f\"Validation examples: {len(en_val)}\")\n",
    "print(\"\\nExample pairs:\")\n",
    "for i in range(3):\n",
    "    print(f\"EN: {en_train[i]}\")\n",
    "    print(f\"DE: {de_train[i]}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dda79a-d0fe-4335-95e2-42194ac11c2c",
   "metadata": {},
   "source": [
    "### 1.2 Build Tokenizers\n",
    "\n",
    "A tokenizer is a crucial tool that acts as a translator. It converts human-readable text (like a sentence) into a format that a computer can understand and process: numerical data.\n",
    "\n",
    "The process of tokenization typically involves two main steps:\n",
    "\n",
    "1. Breaking down the text: The tokenizer takes a long string of text and splits it into smaller, meaningful units called tokens. These tokens can be individual words, characters, or, as in the case of the code you provided, parts of words (subwords).\n",
    "\n",
    "2. Converting to numbers: Each unique token is assigned a unique numerical ID. The model then works with these numbers, which are far easier for a computer to process than raw text.\n",
    "\n",
    "By creating a vocabulary of tokens and their corresponding IDs, the tokenizer allows the model to map between the text it \"sees\" and the numerical representations it can learn from.\n",
    "\n",
    "The provided Python code is a script for creating and training a Byte-Pair Encoding (BPE) tokenizer, a popular subword tokenization method used in many modern language models. The script is building two separate tokenizers: one for English and one for German."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f6b31-2d3d-416b-9e04-1aedd2d35a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizers(en_sentences, de_sentences, vocab_size=8000):\n",
    "    \"\"\"Create BPE tokenizers for English and German\"\"\"\n",
    "\n",
    "    # English tokenizer\n",
    "    en_tokenizer = Tokenizer(BPE())\n",
    "    en_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    en_trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "    )\n",
    "\n",
    "    # Train the English tokenizer\n",
    "    en_tokenizer.train_from_iterator(en_sentences, en_trainer)\n",
    "\n",
    "    # German tokenizer\n",
    "    de_tokenizer = Tokenizer(BPE())\n",
    "    de_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    de_trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "    )\n",
    "\n",
    "    # Train the German tokenizer\n",
    "    de_tokenizer.train_from_iterator(de_sentences, de_trainer)\n",
    "\n",
    "    return en_tokenizer, de_tokenizer\n",
    "\n",
    "en_tokenizer, de_tokenizer = create_tokenizers(en_train, de_train)\n",
    "print(\"Tokenizers created!\")\n",
    "\n",
    "# Test tokenization\n",
    "test_en = \"Hello, how are you?\"\n",
    "test_de = \"Hallo, wie geht es dir?\"\n",
    "print(f\"EN tokens: {en_tokenizer.encode(test_en).tokens}\")\n",
    "print(f\"DE tokens: {de_tokenizer.encode(test_de).tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b40db9-0768-4609-ba01-cdb87bfc5a9a",
   "metadata": {},
   "source": [
    "## Part 2: Transformer Architecture Implementation\n",
    "\n",
    "### 2.1 Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20ba9e4-44f5-4aaf-8324-4800b3ccffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding from the original paper\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_length=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        position = torch.arange(0, max_length).unsqueeze(1).float()\n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *-(math.log(10000.0) / d_model))\n",
    "\n",
    "        # Apply sin to even indices, cos to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Register as buffer (not a parameter)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to input embeddings\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# Test positional encoding\n",
    "pos_enc = PositionalEncoding(d_model=512)\n",
    "test_input = torch.randn(2, 10, 512)  # batch_size=2, seq_len=10, d_model=512\n",
    "output = pos_enc(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f4ca56-6cb9-4291-977d-122e565aa2da",
   "metadata": {},
   "source": [
    "#### 2.2 Multi-Head Attention (Complete Implementation)\n",
    "\n",
    "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
    "\n",
    "We call our particular attention \"Scaled Dot-Product Attention\". The input consists of queries and keys of dimension $d_k$, and values of dimension dv. We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.\n",
    "\n",
    "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$.  The keys and values are also packed together into matrices $K$ and $V$.  We compute the matrix of outputs as:\n",
    "\n",
    "$$\n",
    "   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
    "\n",
    "$$\n",
    "\\mathrm{MultiHead}(Q, K, V) =\n",
    "    \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O \\\\\n",
    "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
    "$$\n",
    "\n",
    "Where the projections are parameter matrices $W^Q_i \\in\n",
    "\\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^K_i \\in\n",
    "\\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^V_i \\in\n",
    "\\mathbb{R}^{d_{\\text{model}} \\times d_v}$ and $W^O \\in\n",
    "\\mathbb{R}^{hd_v \\times d_{\\text{model}}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5d3307-fbc9-4b21-bae3-bb1a6179e5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention from 'Attention is All You Need'\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        # We assume d_v equals d_k\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
    "        \"\"\"Compute scaled dot-product attention\"\"\"\n",
    "        #TODO 1: Implement the attention formula ----------------------------\n",
    "        # scores = Q @ K^T / sqrt(d_k)\n",
    "        scores = 0\n",
    "        #ENDTODO ------------------------------------------------------------\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Apply softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1) # YOUR CODE HERE\n",
    "\n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attention_weights, v) # YOUR CODE HERE\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size, _, d_model = query.size()\n",
    "\n",
    "        # Get sequence lengths - they might be different for query vs key/value\n",
    "        query_len = query.size(1)\n",
    "        key_len = key.size(1)\n",
    "        value_len = value.size(1)\n",
    "\n",
    "        # 1. Linear projections\n",
    "        Q = self.w_q(query)\n",
    "        K = self.w_k(key)\n",
    "        V = self.w_v(value)\n",
    "\n",
    "        # 2. Reshape for multi-head attention\n",
    "        #TODO 2: Implement the multihead attention reshaping ---------------\n",
    "        # Use x.view() to reshape [batch_size, relevant_seq_len, num_heads, d_head]\n",
    "        # and then transpose such that it is [batch_size, num_heads, relevant_seq_len, d_head]\n",
    "        Q = None # YOUR CODE HERE\n",
    "        K = None # YOUR CODE HERE\n",
    "        V = None # YOUR CODE HERE\n",
    "        #ENDTODO ------------------------------------------------------------\n",
    "\n",
    "        # 3. Apply attention\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # 4. Concatenate heads\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, query_len, d_model\n",
    "        )\n",
    "\n",
    "        # 5. Final linear projection\n",
    "        output = self.w_o(attention_output)\n",
    "\n",
    "        return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ac8d24-08ef-4f22-8d31-2016788b7945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test Case Setup: Scaled dot product attention ---\n",
    "\n",
    "# Define small, simple tensors for Q, K, and V\n",
    "# Query: 1 sequence of 4 features\n",
    "# Key/Value: 2 sequences of 4 features\n",
    "\n",
    "d_model = 4  # New d_model\n",
    "num_heads = 1\n",
    "d_k = d_model // num_heads\n",
    "\n",
    "q_org = torch.tensor([[[1.0, 2.0, 3.0, 4.0]]], dtype=torch.float32)  # shape (1, 1, 4)\n",
    "k_org = torch.tensor([[[5.0, 6.0, 7.0, 8.0], [9.0, 10.0, 11.0, 12.0]]], dtype=torch.float32) # shape (1, 2, 4)\n",
    "v_org = torch.tensor([[[13.0, 14.0, 15.0, 16.0], [17.0, 18.0, 19.0, 20.0]]], dtype=torch.float32) # shape (1, 2, 4)\n",
    "\n",
    "multiheadattention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "\n",
    "# Your calculation\n",
    "your_outputs, your_weights = multiheadattention.scaled_dot_product_attention(q_org, k_org, v_org)\n",
    "\n",
    "\n",
    "# Manual Calculation\n",
    "d_k = k_org.size(-1)\n",
    "manual_scores = torch.matmul(q_org, k_org.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "manual_attention_weights = F.softmax(manual_scores, dim=-1)\n",
    "manual_output = torch.matmul(manual_attention_weights, v_org)\n",
    "\n",
    "# Use torch.allclose for a robust comparison of floating-point numbers\n",
    "weights_match = torch.allclose(your_weights, manual_attention_weights)\n",
    "output_match = torch.allclose(your_outputs, manual_output)\n",
    "\n",
    "if weights_match and output_match:\n",
    "    print(\"\\nüéâ Congratulations! Your implementation is correct! üéâ\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Not quite. Compare your output to the step-by-step manual calculation above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6755c588-cc6d-4434-8150-f6853d268576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test Case Setup: Multi head attention ---\n",
    "d_model = 4\n",
    "num_heads = 2\n",
    "d_k = d_model // num_heads\n",
    "\n",
    "# Your calculation\n",
    "multiheadattention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "_, your_attention_weights = multiheadattention(q_org, k_org, v_org)\n",
    "\n",
    "# Manual Calculation\n",
    "Q_projected = multiheadattention.w_q(q_org)\n",
    "K_projected = multiheadattention.w_k(k_org)\n",
    "V_projected = multiheadattention.w_v(v_org)\n",
    "manual_Q_reshaped = Q_projected.view(1, q_org.size(1), num_heads, d_k).transpose(1, 2)\n",
    "manual_K_reshaped = K_projected.view(1, k_org.size(1), num_heads, d_k).transpose(1, 2)\n",
    "manual_V_reshaped = V_projected.view(1, v_org.size(1), num_heads, d_k).transpose(1, 2)\n",
    "_, manual_attention_weights = multiheadattention.scaled_dot_product_attention(manual_Q_reshaped, manual_K_reshaped, manual_V_reshaped, mask=None)\n",
    "\n",
    "weights_match = torch.allclose(your_attention_weights, manual_attention_weights)\n",
    "if weights_match:\n",
    "    print(\"\\nüéâ Congratulations! Your implementation is correct! üéâ\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Not quite. Compare your output to the step-by-step manual calculation above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790bd16-11bc-400b-b54d-d47af1f959f6",
   "metadata": {},
   "source": [
    "### 2.3 Encoder Layer\n",
    "\n",
    "The encoder is composed of a stack of $N=6$ identical layers.\n",
    "\n",
    "Each layer has two sub-layers with residual connection and layer norm.\n",
    "1. A multi-head self-attention mechanism with residual connection and layer norm\n",
    "2. A simple, position-wise fully connected feed-forward network with residual connection and layer norm\n",
    "\n",
    "That is, the output of each sub-layer is $\\mathrm{LayerNorm}(x + \\mathrm{Sublayer}(x))$, where $\\mathrm{Sublayer}(x)$ is the function implemented by the sub-layer itself.  \n",
    "\n",
    "Dropout is applied [(cite)](http://jmlr.org/papers/v15/srivastava14a.html) to the output of each sub-layer, before it is added to the sub-layer input and normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ab80d-366d-4d1b-9f22-34346e6f0064",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Single encoder layer from the transformer\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        #TODO 3: Implement encoder layer forward pass\n",
    "        # 1. Self-attention with residual connection and layer norm\n",
    "        # 2. Feed-forward with residual connection and layer norm\n",
    "        \n",
    "        # Self-attention block\n",
    "        attn_output, _ = self.self_attention(x, x, x, mask)\n",
    "        x = None # YOUR CODE HERE: add & norm\n",
    "        \n",
    "        # Feed-forward block\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = None # YOUR CODE HERE: add & norm\n",
    "        #ENDTODO ------------------------------------------------------------\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405336a8-e5f8-40b1-a158-87527f0ceac5",
   "metadata": {},
   "source": [
    "### 2.4 Decoder Layer\n",
    "\n",
    "The decoder is also composed of a stack of $N=6$ identical layers.\n",
    "\n",
    "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.  Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd2d546-41e2-4d05-836d-5d0cd2a5c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Single decoder layer with masked self-attention and encoder-decoder attention\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.encoder_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        #TODO 4: Implement decoder layer\n",
    "        # 1. Masked self-attention\n",
    "        # 2. Encoder-decoder attention\n",
    "        # 3. Feed-forward\n",
    "        # Each with residual connections and layer norm\n",
    "        \n",
    "        # Masked self-attention\n",
    "        self_attn_output, _ = self.self_attention(x, x, x, tgt_mask)\n",
    "        x = None # YOUR CODE HERE\n",
    "        \n",
    "        # Encoder-decoder attention\n",
    "        enc_attn_output, _ = self.encoder_attention(x, encoder_output, encoder_output, src_mask)\n",
    "        x = None # YOUR CODE HERE\n",
    "        \n",
    "        # Feed-forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = None  # YOUR CODE HERE\n",
    "        #ENDTODO ------------------------------------------------------------\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653de44c-ca93-4ceb-8a6d-0fce56568e0d",
   "metadata": {},
   "source": [
    "### 2.5 Complete Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4029f13f-349f-41af-89a0-6c1617b6bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Complete encoder-decoder transformer\"\"\"\n",
    "\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Embeddings\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "\n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def create_padding_mask(self, seq, pad_token_id=0):\n",
    "        \"\"\"Create mask for padding tokens\"\"\"\n",
    "        return (seq != pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        \"\"\"Create look-ahead mask for decoder\"\"\"\n",
    "        mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "        return mask == 0\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"Encode source sequence\"\"\"\n",
    "        # Embedding + positional encoding\n",
    "        x = self.src_embedding(src) * math.sqrt(self.d_model)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pass through encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, src_mask)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decode(self, tgt, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"Decode target sequence\"\"\"\n",
    "        # Embedding + positional encoding\n",
    "        x = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pass through decoder layers\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def generate(self, tgt, encoder_output, src_mask, tgt_mask):\n",
    "      \"Generate one token at a time\"\n",
    "      x = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
    "      x = self.positional_encoding(x)\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        # Create masks if not provided\n",
    "        if src_mask is None:\n",
    "            src_mask = self.create_padding_mask(src)\n",
    "        if tgt_mask is None:\n",
    "            tgt_mask = self.create_padding_mask(tgt) & self.create_look_ahead_mask(tgt.size(1)).to(tgt.device)\n",
    "\n",
    "        # Encode and decode\n",
    "        encoder_output = self.encode(src, src_mask)\n",
    "        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "        # Project to vocabulary\n",
    "        output = self.output_projection(decoder_output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a18bd-7fa9-4f64-a9b5-64d8fd95b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = en_tokenizer.get_vocab_size()   # Update after tokenizer training\n",
    "tgt_vocab_size = de_tokenizer.get_vocab_size()   # Update after tokenizer training\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    d_ff=2048,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6976f90a-410e-4c05-a895-2b50fb33e23c",
   "metadata": {},
   "source": [
    "## Part 3: Data Processing and Training Setup\n",
    "\n",
    "### 3.1 Create Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23179125-813a-48c7-bb30-7207aef86e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for translation training\"\"\"\n",
    "\n",
    "    def __init__(self, src_sentences, tgt_sentences, src_tokenizer, tgt_tokenizer, max_length=128):\n",
    "        self.src_sentences = src_sentences\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.src_sos_token = src_tokenizer.encode('<sos>').ids\n",
    "        self.src_eos_token = src_tokenizer.encode('<eos>').ids\n",
    "        self.tgt_sos_token = tgt_tokenizer.encode('<sos>').ids\n",
    "        self.tgt_eos_token = tgt_tokenizer.encode('<eos>').ids\n",
    "\n",
    "        self.pad_token_id = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.src_sentences[idx]\n",
    "        tgt_text = self.tgt_sentences[idx]\n",
    "\n",
    "        # 1. Encode source sentence\n",
    "        # 2. Encode target sentence with <sos> and <eos> tokens\n",
    "        # 3. Pad to max_length\n",
    "\n",
    "        # Tokenize source\n",
    "        src_tokens = self.src_tokenizer.encode(src_text).ids\n",
    "        src_tokens = src_tokens[:self.max_length-1]\n",
    "\n",
    "        # Tokenize target (add <sos> at start, <eos> at end)\n",
    "        tgt_tokens = self.tgt_tokenizer.encode(tgt_text).ids\n",
    "        tgt_input = self.tgt_sos_token + tgt_tokens[:self.max_length-2]  # <sos> + tokens\n",
    "        tgt_output = tgt_tokens[:self.max_length-2] + self.tgt_eos_token  # tokens + <eos>\n",
    "\n",
    "        # Pad target sequences\n",
    "        src_tokens = src_tokens + [self.pad_token_id] * max(0, self.max_length - len(src_tokens))\n",
    "        tgt_input = tgt_input + [self.pad_token_id] * max(0, self.max_length - len(tgt_input))\n",
    "        tgt_output = tgt_output + [self.pad_token_id] * max(0, self.max_length - len(tgt_output))\n",
    "\n",
    "        # Assertions to verify lengths\n",
    "        assert len(src_tokens) == self.max_length, f\"Source length mismatch: {len(src_tokens)} != {self.max_length}\"\n",
    "        assert len(tgt_input) == self.max_length, f\"Target input length mismatch: {len(tgt_input)} != {self.max_length}\"\n",
    "        assert len(tgt_output) == self.max_length, f\"Target output length mismatch: {len(tgt_output)} != {self.max_length}\"\n",
    "\n",
    "        return {\n",
    "            'src': torch.tensor(src_tokens[:self.max_length], dtype=torch.long),\n",
    "            'tgt_input': torch.tensor(tgt_input[:self.max_length], dtype=torch.long),\n",
    "            'tgt_output': torch.tensor(tgt_output[:self.max_length], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TranslationDataset(en_train, de_train, en_tokenizer, de_tokenizer)\n",
    "val_dataset = TranslationDataset(en_val, de_val, en_tokenizer, de_tokenizer)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ddcb30-2909-49db-8c7f-6cffc6416a6d",
   "metadata": {},
   "source": [
    "### 3.2 Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5a50d-3147-4913-93c7-967936e259b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        src = batch['src'].to(device)\n",
    "        tgt_input = batch['tgt_input'].to(device)\n",
    "        tgt_output = batch['tgt_output'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(src, tgt_input)\n",
    "\n",
    "        # Compute loss\n",
    "        # Reshape output and target for cross-entropy loss\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        tgt_output = tgt_output.view(-1)\n",
    "\n",
    "        loss = criterion(output, tgt_output)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            src = batch['src'].to(device)\n",
    "            tgt_input = batch['tgt_input'].to(device)\n",
    "            tgt_output = batch['tgt_output'].to(device)\n",
    "\n",
    "            output = model(src, tgt_input)\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            tgt_output = tgt_output.view(-1)\n",
    "\n",
    "            loss = criterion(output, tgt_output)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1cbc56-15e1-4f1d-bc2e-453702cc0a8d",
   "metadata": {},
   "source": [
    "### 3.3 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5868de-565c-44bd-a0d7-e957307f5597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "def train_transformer(num_epochs=3,  patience=10, model_path='model.pt'):\n",
    "    \"\"\"Complete training setup and loop\"\"\"\n",
    "\n",
    "    # Training hyperparameters\n",
    "    num_epochs = num_epochs\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 1e-4  # L2 regularization\n",
    "    grad_clip = 1.0  # Gradient clipping to prevent exploding gradients\n",
    "    warmup_epochs = 5  # Warmup learning rate\n",
    "\n",
    "    # Loss function (ignore padding tokens)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
    "\n",
    "    # Optimizer with learning rate scheduling\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9, weight_decay=weight_decay)\n",
    "\n",
    "    # Learning rate scheduler with warmup\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / warmup_epochs\n",
    "        return 1.0\n",
    "        \n",
    "    warmup_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    main_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs-warmup_epochs)\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validate\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Update learning rate\n",
    "        if epoch < warmup_epochs:\n",
    "            warmup_scheduler.step()\n",
    "        else:\n",
    "            main_scheduler.step()\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        # print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Plot train and val losses\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        epochs_range = list(range(1, len(train_losses) + 1))\n",
    "        plt.plot(epochs_range, train_losses, 'b-o', label='Training Loss', linewidth=2)\n",
    "        plt.plot(epochs_range, val_losses, 'r-s', label='Validation Loss', linewidth=2)\n",
    "        plt.xlabel('Epoch', fontsize=12)\n",
    "        plt.ylabel('Loss', fontsize=12)\n",
    "        plt.title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "        plt.legend(fontsize=11)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            print(f'Validation loss decreased ({best_val_loss:.6f} --> {val_loss:.6f}). Saving model...')\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, model_path)\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "            # Early stopping check\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"\\nEarly stopping triggered at epoch {epoch + 1}\")\n",
    "                print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "                break\n",
    "                \n",
    "    return train_losses, val_losses\n",
    "train_losses, val_losses = train_transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da22cf-6afa-4143-82a6-e1a017ff4d15",
   "metadata": {},
   "source": [
    "### 3.4 Inference and Translation\n",
    "\n",
    "You can download the model trained by Kavi with [this link](https://drive.google.com/file/d/1CnqoS-S9jh5YQzinpOFElpv4bN4AGYgQ/view?usp=sharing). Please put that to the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a9c518-dcca-4ab5-a39f-0722e6b3eeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_src_len = 128\n",
    "\n",
    "# Uncomment this if you download the model with the link above.\n",
    "# model.load_state_dict(torch.load('best_model.pt')['model_state_dict'])\n",
    "\n",
    "def translate_sentence(model, sentence, src_tokenizer, tgt_tokenizer, device, max_length=50):\n",
    "    \"\"\"Translate a single sentence using the trained model\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Encode source sentence\n",
    "        src_tokens = src_tokenizer.encode(sentence).ids\n",
    "        src_len = len(src_tokens)\n",
    "\n",
    "        # Pad source to max_src_len (same as training)\n",
    "        max_src_len = 128\n",
    "        src_tokens = src_tokens[:max_src_len]  # Truncate if too long\n",
    "        padded_src = src_tokens + [0] * (max_src_len - len(src_tokens))  # Pad if too short\n",
    "        src = torch.tensor([padded_src], dtype=torch.long).to(device)\n",
    "\n",
    "        # Create source mask - CRITICAL: mask out padding tokens\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)  # [1, 1, 1, src_len]\n",
    "        # Encode source\n",
    "        encoder_output = model.encode(src, None)\n",
    "\n",
    "        # Initialize decoder input with <sos> token\n",
    "        sos_id = tgt_tokenizer.encode('<sos>').ids[0]\n",
    "        eos_id = tgt_tokenizer.encode('<eos>').ids[0]\n",
    "        \n",
    "        # Initialize decoder input with <sos> token\n",
    "        tgt_input = torch.tensor([[sos_id]], dtype=torch.long).to(device)  # <sos>\n",
    "        generated_tokens = []\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            tgt_len = tgt_input.size(1)\n",
    "            tgt_mask = torch.tril(torch.ones(tgt_len, tgt_len)).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            tgt_mask = tgt_mask.bool()\n",
    "            \n",
    "            # Get decoder output\n",
    "            decoder_output = model.decode(tgt_input, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "            # Get next token probabilities\n",
    "            next_token_logits = model.output_projection(decoder_output[:, -1, :])\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).item()\n",
    "\n",
    "            if next_token == eos_id:\n",
    "                break\n",
    "\n",
    "            # Append the new token to our sequence\n",
    "            generated_tokens.append(next_token)\n",
    "            tgt_input = torch.cat([tgt_input, torch.tensor([[next_token]], device=device)], dim=1)\n",
    "\n",
    "        translation = tgt_tokenizer.decode(generated_tokens)\n",
    "        return translation\n",
    "test_sentence = \"Hello, how are you?\"\n",
    "translation = translate_sentence(model, test_sentence, en_tokenizer, de_tokenizer, device)\n",
    "print(f\"Source: {test_sentence}\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
