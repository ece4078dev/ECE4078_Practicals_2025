{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIwkMy9WnrRn"
   },
   "source": [
    "# <center> Introduction to Reinforcement Learning</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FY8sb3WLnrT6"
   },
   "source": [
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43322,
     "status": "ok",
     "timestamp": 1757480316832,
     "user": {
      "displayName": "Irene JIANG",
      "userId": "15492625806050953964"
     },
     "user_tz": -600
    },
    "id": "xHhQP_yrnrUA",
    "outputId": "eceb91df-36e2-48f2-8599-d5929af37566"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "from Practical11_Support.gym_simple_gridworlds.helper import *\n",
    "from Practical11_Support.gym_simple_gridworlds.envs.grid_env import GridEnv\n",
    "from Practical11_Support.gym_simple_gridworlds.envs.grid_2dplot import *\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8a82a39e"
   },
   "source": [
    "# The Grid World environment\n",
    "\n",
    "Recall the grid in which our robot lives\n",
    "\n",
    "![GridWorldExample.png](https://i.postimg.cc/5tMM5vqf/Grid-World-Example.png)\n",
    "\n",
    "- The states $s \\in \\mathcal{S}$ correspond to locations in the grid. Each location has also a cell index associated to it, e.g., cell index 4 is associated to location (row=1,col=0)\n",
    "- The robot can move up, down, left, or right. Actions correpond to unit increments or decrements in the specified direction.\n",
    "    - Up : (-1,0)\n",
    "    - Down: (1,0)\n",
    "    - Left: (0,-1)\n",
    "    - Right: (0, 1)\n",
    "- Each action is represented by a number. Action (Up) is represented by 0, (Down) by 1, (Left) by 2 and, finally, (Right) by 3. No actions are available at a terminal state\n",
    "- Discount factor $\\gamma = 0.99$ (class attribute ``gamma=0.99``)\n",
    "- Stochastic transition matrix (class attribute ``noise=0.2``)\n",
    "- Rewards are only obtained at terminal states (class attribute ``living_reward=-0.04``)\n",
    "\n",
    "This environment is represented with the class ``GridEnv``. **To a look at the attributes of this class, place the cursor somewhere on the class' name and hit SHIFT+TAB (local Jupyter Notebook) or hover your mouse over it (Colab). If there's a + button at the top of the popup tooltip, this means the documentation spans a few lines, click it to show the full docstring, then scroll up.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nwSIRgDQPMs"
   },
   "source": [
    "# Activity 1. Elements of an MDP (Grid World Example)\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this activity, you will be able to:\n",
    "\n",
    "* Describe the components of a Markov Decision Process (MDP).\n",
    "\n",
    "* Explore state and action spaces in a grid environment.\n",
    "\n",
    "* Understand transition dynamics and rewards.\n",
    "\n",
    "* Observe how a policy influences agent behavior.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjXoXqOnnrXQ"
   },
   "source": [
    "## Create Environment and Explore its Attributes\n",
    "\n",
    "The noise parameter corresponds to the probability of a change of direction when an action is taken (e.g., going left/right when agent decides to move up/down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iv58f67EnrXZ"
   },
   "outputs": [],
   "source": [
    "# Create a Grid World instance\n",
    "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=-0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzBYFM7WnrXb"
   },
   "source": [
    "### State and Action Spaces\n",
    "\n",
    "Let's take a look at the state and action spaces of our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1757480316849,
     "user": {
      "displayName": "Irene JIANG",
      "userId": "15492625806050953964"
     },
     "user_tz": -600
    },
    "id": "BJhhxR-HnrXw",
    "outputId": "8b468089-fc14-40ac-c3c7-0048b21915b8"
   },
   "outputs": [],
   "source": [
    "# State (or observation) space\n",
    "print(grid_world.observation_space)\n",
    "print(grid_world.get_states())\n",
    "print()\n",
    "\n",
    "# Action space\n",
    "print(grid_world.action_space)\n",
    "print(grid_world.get_actions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhVeMbYgsOoK"
   },
   "source": [
    "Discrete(11) ‚Üí there are 11 valid states in this GridWorld.\n",
    "\n",
    "The environment encodes each state as an integer index 0 ‚Ä¶ 10.\n",
    "\n",
    "Why 11 instead of 12 (3√ó4 grid)?\n",
    "\n",
    "Because walls / blocked cells are not part of the state space.\n",
    "\n",
    "Only usable positions are numbered.\n",
    "\n",
    "So each number corresponds to a cell (row, col) in the grid, except walls.\n",
    "\n",
    "‚úÖ This is a common OpenAI Gym convention: states are represented by integers instead of (row, col) tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mp2FXl9Nsb97"
   },
   "source": [
    "Discrete(4) ‚Üí there are 4 possible actions.\n",
    "\n",
    "They‚Äôre encoded as integers [0, 1, 2, 3].\n",
    "\n",
    "Typically:\n",
    "\n",
    "0 = North (Up)\n",
    "\n",
    "1 = South (Down)\n",
    "\n",
    "2 = West (Left)\n",
    "\n",
    "3 = East (Right)\n",
    "\n",
    "(This exact mapping depends on the implementation of GridEnv, but the idea is the same.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwWFnDIKnrX8"
   },
   "source": [
    "### Transition Function\n",
    "\n",
    "Let's take a look at the current state transition function. Some things to keep in mind regarding the transition function:\n",
    "\n",
    "1. Given that $\\mathcal{T}: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\mathbb{R}$, the ``state_transitions`` attribute of the class ``GridEnv`` corresponds to a 3-Dimensional numpy array of size $11\\times4\\times11$.\n",
    "2. With a noise attribute set to 0.2, at state 5, if the agent chooses to move up, it will end up at:\n",
    "    - state 2 with $80\\%$ probability,\n",
    "    - state 6 with $10\\%$ probability, or\n",
    "    - state 5 with $10\\%$ probability\n",
    "\n",
    "![GridWorldExample.png](https://i.postimg.cc/5tMM5vqf/Grid-World-Example.png)\n",
    "\n",
    "![Screenshot from 2025-09-04 00-40-26.png](https://i.postimg.cc/fTr59Ls8/stuff.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1757480316854,
     "user": {
      "displayName": "Irene JIANG",
      "userId": "15492625806050953964"
     },
     "user_tz": -600
    },
    "id": "cGOVNUHQnrYE",
    "outputId": "e5595add-c42b-4fcd-8135-ca5e438df289"
   },
   "outputs": [],
   "source": [
    "# at state 5 the agent takes action 0 (going up)\n",
    "print(grid_world.state_transitions[5,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fjm1IpAvsjjb"
   },
   "source": [
    "grid_world.state_transitions[5,0]\n",
    "\n",
    "First index = state (here 5);\n",
    "Second index = action (here 0, which probably means ‚ÄúUp/North‚Äù)\n",
    "\n",
    "The returned row = probability distribution over next states.\n",
    "[0.  0.  0.8  0.  0.  0.1  0.1  0.  0.  0.  0.]\n",
    " ‚Üë  ‚Üë    ‚Üë              ‚Üë   ‚Üë\n",
    " s0 s1   s2             s5  s6\n",
    "\n",
    "From state 5, action 0 leads to:\n",
    "\n",
    "State 2 with prob 0.8 (the intended ‚Äúup‚Äù move)\n",
    "\n",
    "State 5 with prob 0.1 (blocked or slip ‚Üí stays in place)\n",
    "\n",
    "State 6 with prob 0.1 (slip to the side)\n",
    "\n",
    "All other states ‚Üí probability 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S922ir8drndS"
   },
   "source": [
    "General rule\n",
    "\n",
    "For any (s, a):\n",
    "\n",
    "ùì£‚üÆs , a, s'‚üØ = ‚Ñô‚üÆ s'| s, a‚üØ = grid_world.state_transitions[s, a][s']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG5N7pTgnrYH"
   },
   "source": [
    "### Living Reward and Reward Function\n",
    "\n",
    "Let's now take a quick look at the living reward (i.e., running cost) and reward function $\\mathcal{R}: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$.\n",
    "\n",
    "1. Living reward corresponds to the attribute ``living_rewards`` of the class ``GridEnv`` and is represented as an 1-Dimensional numpy array\n",
    "2. The reward function corresponds to the attribute ``rewards`` of the class ``GridEnv`` and is also represented as a 2-Dimensional numpy array of size $11\\times4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1757480316858,
     "user": {
      "displayName": "Irene JIANG",
      "userId": "15492625806050953964"
     },
     "user_tz": -600
    },
    "id": "mw7kWhW8nrYH",
    "outputId": "22c19c40-e3c2-4c13-dfa9-d8f85d135663"
   },
   "outputs": [],
   "source": [
    "# Living rewards\n",
    "print(\"Living rewards for all states:\\n{}\\n\".format(grid_world.immediate_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZMroUOst3Jp"
   },
   "source": [
    "This is a 1-D array of size = number of states (11).\n",
    "\n",
    "Each entry = reward for being in that state at a step (before considering action outcomes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1757480316865,
     "user": {
      "displayName": "Irene JIANG",
      "userId": "15492625806050953964"
     },
     "user_tz": -600
    },
    "id": "ow_4EWhztVTp",
    "outputId": "0bd2641e-d8d2-46d8-d541-fa4751a40285"
   },
   "outputs": [],
   "source": [
    "# Reward function, i.e., the expected reward for taking action a at state s\n",
    "print(\"Reward function for all state-action pairs:\\n{}\\n\".format(grid_world.rewards))\n",
    "print(\"The expected reward at state 5 if agent chooses to move right is: {}\".format(grid_world.rewards[5,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFlAFq8bt9BP"
   },
   "source": [
    "This is a 2-D array of shape (n_states, n_actions).\n",
    "\n",
    "Entry rewards[s, a] = expected reward if you take action a in state s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmN9N1J0DsVN"
   },
   "source": [
    "### **TODO** (Flux Quiz 2): what is the expected reward at state 2 if the agent chooses to move right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Puxe1hrynrYK"
   },
   "source": [
    "### Policy\n",
    "\n",
    "Let's see the path and total reward of an agent moving on our grid world according to the following policy $\\pi$\n",
    "\n",
    "![example_policy.png](https://i.postimg.cc/pLjHnkj0/example-policy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkCYTTSSnrYM"
   },
   "outputs": [],
   "source": [
    "# We represent this policy as a 2-Dimensional numpy array\n",
    "# This is a fixed, hand-coded policy\n",
    "policy_matrix = np.array([[3,      3,  3,  -1],\n",
    "                          [0, np.nan,  0,  -1],\n",
    "                          [0,      2,  0,   2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1757480316894,
     "user": {
      "displayName": "Irene JIANG",
      "userId": "15492625806050953964"
     },
     "user_tz": -600
    },
    "id": "uhXS8_t2nrYO",
    "outputId": "b72eb5aa-d84e-4e42-f8b2-5db2c5ceb2de"
   },
   "outputs": [],
   "source": [
    "print(grid_world.grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qw1wo7-0nrYV"
   },
   "source": [
    "Let's now apply this fixed, hand-coded policy and observe the agent's behavior (blue dot in the figure shown below).\n",
    "\n",
    "In Grid World, terminal states are the goal (reward +1) and the trap (reward ‚Äì1).\n",
    "\n",
    "Once the agent reaches one of those, the environment sets done=True.\n",
    "\n",
    "That‚Äôs what stops the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    },
    "executionInfo": {
     "elapsed": 1341,
     "status": "ok",
     "timestamp": 1757480318234,
     "user": {
      "displayName": "Irene JIANG",
      "userId": "15492625806050953964"
     },
     "user_tz": -600
    },
    "id": "DZ84dyGjnrYd",
    "outputId": "acd9563b-408d-4e3b-88bf-2f12848ecbff"
   },
   "outputs": [],
   "source": [
    "# Create a Grid World instance\n",
    "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=-0.04)\n",
    "s_x, s_y = get_state_to_plot(grid_world)\n",
    "\n",
    "# We can visualize our grid world using the render() function\n",
    "fig, ax = grid_world.render()\n",
    "agent, = ax.plot([], [], 'o', color='b', linewidth=6)\n",
    "reward_text = ax.text(0.02, 0.95, '', transform=ax.transAxes)\n",
    "\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "cur_state = grid_world.cur_state # where the agent starts\n",
    "path_to_plot = []\n",
    "\n",
    "while not done:\n",
    "    _, cur_reward, done, _ = grid_world.step(int(policy_matrix[cur_state[0], cur_state[1]]))\n",
    "    cur_state = grid_world.cur_state\n",
    "    n_x, n_y = get_state_to_plot(grid_world)\n",
    "    cumulative_reward += cur_reward\n",
    "    path_to_plot.append([cumulative_reward, n_x, n_y])\n",
    "\n",
    "def init():\n",
    "    agent.set_data([s_x + 0.5], [s_y + 0.5])\n",
    "    reward_text.set_text('')\n",
    "    return agent, reward_text\n",
    "\n",
    "def animate(i):\n",
    "    if i < len(path_to_plot):\n",
    "        r, n_x, n_y = path_to_plot[i]\n",
    "        agent.set_data([n_x + 0.5], [n_y + 0.5])\n",
    "        reward_text.set_text('Cumulative reward: %.2f' % r)\n",
    "    return agent, reward_text\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(path_to_plot), blit=False, interval=500, init_func=init,\n",
    "                              repeat=False)\n",
    "\n",
    "plt.close('all')\n",
    "display(HTML(f\"<div align=\\\"center\\\">{ani.to_jshtml()}</div>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0mO2xNDnrYy"
   },
   "outputs": [],
   "source": [
    "### Helper Function ###\n",
    "def policy_evaluation(grid_env, policy, plot=False, threshold=0.00001):\n",
    "\n",
    "    \"\"\"\n",
    "    This function computes the value function for a policy pi in a given environment grid_env.\n",
    "\n",
    "    :param grid_env (GridEnv): MDP environment\n",
    "    :param policy (dict - stochastic form): Policy being evaluated\n",
    "    :return: (dict) State-values for all non-terminal states\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtain list of all states in environment\n",
    "    v = {s: 0.0 for s in grid_env.get_states()}\n",
    "    theta = threshold\n",
    "    delta = 1000\n",
    "\n",
    "    while delta > theta:\n",
    "        delta = 0.0\n",
    "        # For all states\n",
    "        for s in v.keys():\n",
    "\n",
    "            old_v = v[s]\n",
    "            new_v = 0\n",
    "\n",
    "            # For all actions\n",
    "            for a, probability_a in policy[s].items():\n",
    "                discounted_v = 0\n",
    "\n",
    "                # For all states that are reachable from s with action a\n",
    "                for s_next in grid_env.get_states():\n",
    "                    #TODO 1: Compute discounted sum of state values for all successor states ---------\n",
    "                    discounted_v += 0\n",
    "                    #ENDTODO -------------------------------------------------------------------------\n",
    "\n",
    "                #TODO 2: Compute expectation over all actions ------------------------------------\n",
    "                new_v += 0\n",
    "                #ENDTODO -------------------------------------------------------------------------\n",
    "\n",
    "            v[s] = new_v\n",
    "            delta = max(delta, np.abs(old_v - new_v))\n",
    "\n",
    "    if plot:\n",
    "        plot_value_function(grid_env, v)\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPxrnZjWqV_q"
   },
   "source": [
    "# Activity 2 Model-Free RL:Q-Learning\n",
    "What if the Transition and Reward Function are Unknown?\n",
    "\n",
    "How to evaluate a policy without a model.\n",
    "\n",
    "Let's now find an *approximately* optimal policy using the off-policy control method Q-learning.\n",
    "\n",
    "To help during the learning, we have added a lambda function that iteratively decreases epsilon. Our agent will strongly explore the environment at first to then swicth into exploitation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZk423lbqV_r"
   },
   "source": [
    "#  The Grid World environment\n",
    "\n",
    "Recall the grid in which our robot lives\n",
    "\n",
    "![GridWorldExample.png](https://i.postimg.cc/5tMM5vqf/Grid-World-Example.png)\n",
    "\n",
    "- The states $s \\in \\mathcal{S}$ correspond to locations in the grid. Each location has also a cell index associated to it, e.g., cell index 4 is associated to location (row=1,col=0)\n",
    "- The robot can move up, down, left, or right. Actions correpond to unit increments or decrements in the specified direction.\n",
    "    - Up : (-1,0)\n",
    "    - Down: (1,0)\n",
    "    - Left: (0,-1)\n",
    "    - Right: (0, 1)\n",
    "- Each action is represented by a number. Action (Up) is represented by 0, (Down) by 1, (Left) by 2 and, finally, (Right) by 3. No actions are available at a terminal state\n",
    "- Discount factor $\\gamma = 0.99$ (class attribute ``gamma=0.99``)\n",
    "- Stochastic transition matrix (class attribute ``noise=0.2``)\n",
    "- Rewards are only obtained at terminal states (class attribute ``living_reward=-0.04``)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2S0rW-DCqV_r"
   },
   "source": [
    "Here is our implementation of the q-learning algorithm shown below\n",
    "\n",
    "![q-learning.png](https://i.postimg.cc/8z70Yv5C/q-learning.png)\n",
    "\n",
    "**Complete the missing steps**:\n",
    "- Choose an action using an $\\epsilon$-greedy policy (use the function ``get_egreedy_action(.)`` we tested in Section 3.1)\n",
    "- Update our q-function using a greedy (max) policy (use ``q_function[cur_state][action]`` to index our q-function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_1QYlfMqV_r"
   },
   "source": [
    "## TODO (Flux Quiz 3) Complete the missing steps:\n",
    "- Choose an action using an $\\epsilon$-greedy policy (use the function ``get_egreedy_action(.)``)\n",
    "- Update our q-function using a greedy (max) policy (use ``q_function[cur_state][action]`` to index our q-function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22x8SFrxqV_r"
   },
   "source": [
    "**Keep in Mind**: Correspondance between the mathematical notation and implemented code\n",
    "\n",
    "|                 |                            |                  |\n",
    "| :-------------- | -------------------------: | ---------------: |\n",
    "|                 | **Variable/Attribute**     | **Type**         |\n",
    "| $\\epsilon$      | `epsilon_by_episode`       | `float`          |\n",
    "| $\\alpha$        | `alpha`                    | `float`          |\n",
    "| $\\gamma$        | `grid_world.gamma`         | `float`          |\n",
    "| $\\hat{q}(s, a)$ | `q_function[idx_s][idx_a]` | `dict` of `dict` |\n",
    "| $s$             | `cur_state`                | `int`            |\n",
    "| $r$             | `reward`                   | `int`            |\n",
    "| $s^{\\prime}$    | `next_state`               | `int`            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "029RZNhFqV_r"
   },
   "source": [
    "### Step 1 initialize the Q-table with values of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1757480318243,
     "user": {
      "displayName": "Irene JIANG",
      "userId": "15492625806050953964"
     },
     "user_tz": -600
    },
    "id": "9pO4biVAqV_r",
    "outputId": "545f2c56-d2ed-4850-f79f-3ce445c6b1aa"
   },
   "outputs": [],
   "source": [
    "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=-0.04)\n",
    "states = grid_world.get_states()\n",
    "actions = grid_world.get_actions()\n",
    "# initialize the Q-table with values of 0.\n",
    "q_function = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "for s in states:\n",
    "    for a in actions:\n",
    "        q_function[s][a] = 0.0\n",
    "\n",
    "def print_q_table(q_function, actions):\n",
    "    print(\"State\".ljust(12) + \"\".join([f\"A{a}\".rjust(12) for a in actions]))\n",
    "    print(\"-\" * (12 + 12*len(actions)))\n",
    "    for s, q_vals in q_function.items():\n",
    "        row = str(s).ljust(12) + \"\".join([f\"{q_vals[a]:10.3f}  \" for a in actions])\n",
    "        print(row)\n",
    "\n",
    "print(\"\\n=== Initial Q-Table (all zeros) ===\")\n",
    "print_q_table(q_function, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRkkfy7oqV_r"
   },
   "source": [
    "### Step 2: Choose an action using the epsilon-greedy strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3awEjgYcNDg"
   },
   "source": [
    "\n",
    "#### Define $\\epsilon$-Greedy Policy\n",
    "Epsilon-Greedy is the training policy that handles the exploration/exploitation trade-off.\n",
    "\n",
    "The idea with Epsilon Greedy:\n",
    "\n",
    "- With probability **1‚Ää-‚Ää…õ** : **we do exploitation** (aka our agent selects the action with the highest state-action pair value).\n",
    "\n",
    "- With probability **…õ**: **we do exploration** (trying random action).\n",
    "\n",
    "And as the training goes, we progressively reduce the epsilon value since we will need less and less exploration and more exploitation.\n",
    "\n",
    "\n",
    "$\\epsilon$-greedy policy is formally defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\pi(a|s) =\n",
    "    \\begin{cases}\n",
    "        1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}|},&  \\text{if } a^* = \\arg\\max_{a \\in \\mathcal{A}} q_\\pi(s,a)\\\\\n",
    "        \\frac{\\epsilon}{|\\mathcal{A}|}, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let's see how the agent behaves when it follows an $\\epsilon$-greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r3dXNR17cUbO"
   },
   "outputs": [],
   "source": [
    "def get_egreedy_action(grid_env, state, q_value, epsilon):\n",
    "    \"\"\"\n",
    "    Select action to execute at a given state under an epsilon-greedy policy\n",
    "    :param grid_env (GridEnv): Grid world environment\n",
    "    :param state (int): Location in grid for which next action is going to be choosen\n",
    "    :param q_value (dict): Action-value function\n",
    "    :param epsilon (float): Randomness threshold used to choose action\n",
    "    \"\"\"\n",
    "\n",
    "    rand_n = np.random.random()\n",
    "    if rand_n <= epsilon:\n",
    "        # Explore: pick a random action\n",
    "        return grid_env.action_space.sample()\n",
    "    else:\n",
    "        # Exploit: pick the best action\n",
    "        actions = list(q_value[state].keys())\n",
    "        return actions[np.argmax(list(q_value[state].values()))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nD7nQKMLqV_s"
   },
   "source": [
    "At the beginning of the training, the probability of doing exploration will be huge since …õ is very high, so most of the time, we‚Äôll explore.\n",
    "\n",
    "But as the training goes on, and consequently our Q-table gets better and better in its estimations, we progressively reduce the epsilon value since we will need less and less exploration and more exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRLxwkHRqV_s"
   },
   "source": [
    "epsilon_by_episode (the lambda function)\n",
    "\n",
    "- Defines how Œµ changes with episode index:\n",
    "\t‚Äã\n",
    "- This is an exponential decay schedule.\n",
    "\n",
    "- Early episodes: Œµ is close to 1.0 (explore).\n",
    "\n",
    "- Later episodes: Œµ gets closer to 0.001 (exploit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Svb0-4WUqV_s"
   },
   "outputs": [],
   "source": [
    "min_epsilon=0.001\n",
    "max_epsilon=1.0\n",
    "epsilon_decay = 80.0\n",
    "epsilon_by_episode = lambda ep_idx: (min_epsilon\n",
    "         + (max_epsilon - min_epsilon)\n",
    "         * math.exp (-1 * ep_idx/epsilon_decay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "executionInfo": {
     "elapsed": 83,
     "status": "ok",
     "timestamp": 1757480318332,
     "user": {
      "displayName": "Irene JIANG",
      "userId": "15492625806050953964"
     },
     "user_tz": -600
    },
    "id": "1BWF6vx5qV_s",
    "outputId": "d7a56136-46b1-4e22-a9b4-8367422b70d6"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "ax.plot([epsilon_by_episode(i) for i in range(500)])\n",
    "ax.set_xlabel(\"Num. episodes\")\n",
    "ax.set_ylabel(\"Epsilon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Atll4Z774gri"
   },
   "source": [
    "#### Define the greedy policy\n",
    "Remember we have two policies since Q-Learning is an **off-policy** algorithm. This means we're using a **different policy for acting and updating the value function**.\n",
    "\n",
    "- Epsilon greedy policy (acting policy)\n",
    "- Greedy policy (updating policy)\n",
    "\n",
    "Greedy policy will also be the final policy we'll have when the Q-learning agent will be trained. The greedy policy is used to select an action from the Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3SCLmLX5bWG"
   },
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "  # Exploitation: take the action with the highest state, action value\n",
    "  action = np.argmax(Qtable[state])\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "8YQD1qqrqV_s",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def q_learning(grid_env, alpha=0.1, min_epsilon=0.01, max_epsilon=1.0,\n",
    "               epsilon_decay = 80.0, n_episodes=500):\n",
    "    \"\"\"\n",
    "    This function computes an approximately optimal policy using q-learning\n",
    "\n",
    "    :param grid_env (GridEnv): MDP environment\n",
    "    :param alpha (float): step-size\n",
    "    :param epsilon (float): value used during e-greedy action selection\n",
    "    :return: (dict) State-values for all non-terminal states\n",
    "    \"\"\"\n",
    "\n",
    "    # This lambda function iteratively decreases epsilon\n",
    "    epsilon_by_episode = lambda ep_idx: min_epsilon + (max_epsilon - min_epsilon) * math.exp (-1 * ep_idx/epsilon_decay)\n",
    "\n",
    "    # Obtain list of all states in environment\n",
    "    states = grid_env.get_states()\n",
    "    actions = grid_env.get_actions()\n",
    "    q_function = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "    # Initialize q_function arbitrarily\n",
    "    for s in states:\n",
    "        for a in actions:\n",
    "            q_function[s][a] = 0\n",
    "\n",
    "    episode_returns = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        cur_state = grid_env.reset()\n",
    "        done = False\n",
    "        epsilon = epsilon_by_episode(i_episode)\n",
    "        G = 0.0\n",
    "        while not done:\n",
    "            #TODO 1: Complete off-policy action selection (e-greedy)-----------\n",
    "            #Choose the action using epsilon greedy policy\n",
    "            action = None\n",
    "            #ENDTODO ----------------------------------------------------------\n",
    "\n",
    "            #Take action At and observe Rt+1 and St+1\n",
    "            #Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "            next_state, reward, done,_ = grid_env.step(action)\n",
    "            q_next_state = list(q_function[next_state].values())\n",
    "\n",
    "            #TODO 2: Complete update of q-function -----------------------------\n",
    "            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "            q_function[cur_state][action] += 0\n",
    "            #ENDTODO ------------------------------------------------------------\n",
    "            G += reward\n",
    "            cur_state=next_state\n",
    "        episode_returns.append(G)\n",
    "\n",
    "    return q_function, decode_policy(grid_env, q_function), episode_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 922
    },
    "executionInfo": {
     "elapsed": 418,
     "status": "ok",
     "timestamp": 1757480493569,
     "user": {
      "displayName": "Irene JIANG",
      "userId": "15492625806050953964"
     },
     "user_tz": -600
    },
    "id": "P7umWbegqV_s",
    "outputId": "198ecadc-e4e6-463b-9bad-63e488300b32"
   },
   "outputs": [],
   "source": [
    "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=-0.04)\n",
    "q_function, q_learning_policy, _ = q_learning(grid_world)\n",
    "\n",
    "plot_policy(grid_world, q_learning_policy)\n",
    "print(q_learning_policy)\n",
    "\n",
    "# Compute value function for q_learning policy\n",
    "q_policy_state_values = policy_evaluation(grid_world, encode_policy(grid_world, q_learning_policy))\n",
    "plot_value_function(grid_world, q_policy_state_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDubUWv1e-Zl"
   },
   "source": [
    "Let's see what our Q-Learning table looks like now üëÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1757480318760,
     "user": {
      "displayName": "Irene JIANG",
      "userId": "15492625806050953964"
     },
     "user_tz": -600
    },
    "id": "3noCDnhbeITy",
    "outputId": "39b1a9b7-f87d-446d-e64f-bcc022259c51"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "states  = grid_world.get_states()   # e.g., [(0,0),(0,1),...]\n",
    "actions = grid_world.get_actions()  # e.g., [0,1,2,3]\n",
    "\n",
    "state_to_idx  = {s: i for i, s in enumerate(states)}\n",
    "action_to_idx = {a: j for j, a in enumerate(actions)}\n",
    "\n",
    "Q = np.zeros((len(states), len(actions)))\n",
    "for s in states:\n",
    "    for a in actions:\n",
    "        Q[state_to_idx[s], action_to_idx[a]] = q_function[s][a]\n",
    "\n",
    "print(Q)  # rows = states (in the order of grid_env.get_states()), cols = actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An ablation of $\\epsilon$ on $\\epsilon$-greedy policy\n",
    "\n",
    "Train Q-learning with three Œµ settings and compare policies/returns:\n",
    "- (A) Œµ = 0.9 constant, dubbed `high`\n",
    "- (B) Œµ = 0.1 constant, dubbed `low`\n",
    "- (C) Œµ decays from 1.0 ‚Üí 0.01, dubbed `decay`\n",
    "\n",
    "Goal: Compare policies and returns for three Œµ schedules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_q_with_schedule(schedule: str):\n",
    "    env = GridEnv(gamma=0.99, noise=0.2, living_reward=-0.04)\n",
    "    if schedule == \"high\":\n",
    "        return q_learning(env, n_episodes=500, min_epsilon=0.9, max_epsilon=0.9, epsilon_decay=1.0)\n",
    "    if schedule == \"low\":\n",
    "        return q_learning(env, n_episodes=500, min_epsilon=0.1, max_epsilon=0.1, epsilon_decay=1.0)\n",
    "    # default decay\n",
    "    return q_learning(env, n_episodes=500, min_epsilon=0.01, max_epsilon=1.0, epsilon_decay=80.0)\n",
    "\n",
    "Q_high, _, ret_high   = run_q_with_schedule(\"high\")\n",
    "Q_low, _, ret_low     = run_q_with_schedule(\"low\")\n",
    "Q_decay, _, ret_decay = run_q_with_schedule(\"decay\")\n",
    "\n",
    "grid_env = GridEnv(gamma=0.9, noise=0.2, living_reward=-0.04)\n",
    "\n",
    "for (Q_, name) in zip([Q_high, Q_low, Q_decay], [\"High\", \"Low\", \"Decay\"]):\n",
    "    pol = decode_policy(grid_env, Q_)\n",
    "    print(f\"{name} Œµ policy:\")\n",
    "    plot_policy(grid_env, pol)\n",
    "    plt.show()\n",
    "\n",
    "# Simple running-mean curves (optional)\n",
    "def running_mean(x, N=20):\n",
    "    if len(x) < N:\n",
    "        return np.array(x)\n",
    "    return np.convolve(x, np.ones(N)/N, mode='valid')\n",
    "\n",
    "plt.figure(); plt.plot(running_mean(ret_high)); plt.title('Running mean returns ‚Äì High Œµ')\n",
    "plt.figure(); plt.plot(running_mean(ret_low)); plt.title('Running mean returns ‚Äì Low Œµ')\n",
    "plt.figure(); plt.plot(running_mean(ret_decay)); plt.title('Running mean returns ‚Äì Decay Œµ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. High Œµ (0.9 constant)\n",
    "\n",
    "- Curve stays around low/negative returns with lots of fluctuation.\n",
    "\n",
    "- Explanation: the agent is almost always exploring randomly, so it never really settles on a good policy. That‚Äôs why it looks noisy and hovers near baseline performance.\n",
    "\n",
    "2. Low Œµ (0.1 constant)\n",
    "\n",
    "- Curve quickly climbs to higher positive returns and then oscillates around a stable level.\n",
    "\n",
    "- Explanation: the agent exploits early, so it finds a decent policy fast. But with little exploration, it may converge to a suboptimal but consistent policy.\n",
    "\n",
    "3. Decay Œµ (1.0 ‚Üí 0.01)\n",
    "\n",
    "- Curve starts poor (random exploration), then steadily improves, and finally stabilizes near high returns.\n",
    "\n",
    "- Explanation: the agent explores widely at first, then gradually exploits, which balances learning and convergence. This is typically the best long-term strategy."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
