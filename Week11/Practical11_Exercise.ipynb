{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb23405-dae2-42a0-acc9-901980635bf7",
   "metadata": {},
   "source": [
    "# Practical 11 : Q-Learning & Value-Based Methods (MC vs TD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455e7afa-c4f1-4b58-9718-c68dc432ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import sys, os\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "from Practical11_Support.gym_simple_gridworlds.helper import *\n",
    "from Practical11_Support.gym_simple_gridworlds.envs.grid_env import GridEnv\n",
    "from Practical11_Support.gym_simple_gridworlds.envs.grid_2dplot import *\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5e68f0-4027-4c43-907a-11a140b1669f",
   "metadata": {},
   "source": [
    "## After-Class Exercise (Student): Implement MC & TD(0) Prediction\n",
    "\n",
    "### The Grid World environment\n",
    "\n",
    "Recall the grid in which our robot lives\n",
    "\n",
    "![GridWorldExample.png](https://i.postimg.cc/5tMM5vqf/Grid-World-Example.png)\n",
    "\n",
    "- The states $s \\in \\mathcal{S}$ correspond to locations in the grid. Each location has also a cell index associated to it, e.g., cell index 4 is associated to location (row=1,col=0)\n",
    "- The robot can move up, down, left, or right. Actions correpond to unit increments or decrements in the specified direction.\n",
    "    - Up : (-1,0)\n",
    "    - Down: (1,0)\n",
    "    - Left: (0,-1)\n",
    "    - Right: (0, 1)\n",
    "- Each action is represented by a number. Action (Up) is represented by 0, (Down) by 1, (Left) by 2 and, finally, (Right) by 3. No actions are available at a terminal state\n",
    "- Discount factor $\\gamma = 0.99$ (class attribute ``gamma=0.99``)\n",
    "- Stochastic transition matrix (class attribute ``noise=0.2``)\n",
    "- Rewards are only obtained at terminal states (class attribute ``living_reward=-0.04``)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe922eaf-44d1-4b94-8860-a977443ef787",
   "metadata": {},
   "source": [
    "### Generate episode\n",
    "Let's first define the helper method generate_episode(.). It samples an episode i.e., a sequence of ( s,a,r,s′ ) tuples, from a given policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a45d9-3249-4ce0-befa-4597d9837756",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample = namedtuple('Sample', ['state', 'action', 'reward', 'next_state'])\n",
    "\n",
    "def generate_episode(grid_env, policy):\n",
    "    \"\"\"\n",
    "    Generate an episode of experiences in environment under a given policy\n",
    "    :param grid_env (GridEnv): Environment\n",
    "    :param policy (dict of probabilites): Policy used to sample actions\n",
    "\n",
    "    :return List(Sample) Complete episode\n",
    "    \"\"\"\n",
    "    episode = []\n",
    "\n",
    "    # Reset the environment to a random initial state\n",
    "    state = grid_env.reset()\n",
    "\n",
    "    # Set flag to indicate whether episode has ended\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Get actions available at current state\n",
    "        all_actions = list(policy[state].keys())\n",
    "        # Get action probabilities\n",
    "        all_probabilities = np.array(list(policy[state].values()))\n",
    "        # Sample an action from policy\n",
    "        action = np.random.choice(all_actions, 1, p=all_probabilities)[0]\n",
    "\n",
    "        next_state, reward, done, info = grid_env.step(action)\n",
    "        episode.append(Sample(state, action, reward, next_state))\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6939d052-ab73-4b0a-b2c6-ffb7bcedcb74",
   "metadata": {},
   "source": [
    "Use the same fixed policy for both (e.g., uniform random)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac23050-4c16-42e0-8a1f-fa5e335cc4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a uniform-random stochastic policy (state-id → {action: prob})\n",
    "def make_uniform_random_policy(env):\n",
    "    acts = env.get_actions()\n",
    "    prob = 1.0 / len(acts)\n",
    "    policy = {}\n",
    "\n",
    "    for sid in env.get_states(): # state ids as keys\n",
    "        r, c = np.argwhere(env.grid == sid)[0]\n",
    "        if env.is_terminal_state(r, c):\n",
    "            policy[sid] = {}\n",
    "        else:\n",
    "            policy[sid] = {a: prob for a in acts}\n",
    "    return policy\n",
    "    \n",
    "print(\"building uniform random policy.\")\n",
    "grid_world = GridEnv(gamma=0.99, noise=0.2, living_reward=-0.04)\n",
    "pi_random = make_uniform_random_policy(grid_world)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e94d58-a424-4d65-b505-3649705dfa08",
   "metadata": {},
   "source": [
    "### Implement **MC prediction** (first-visit)  \n",
    "**Monte-Carlo Method**\n",
    "\n",
    "Now, under the assumption that $\\mathcal{T}(s,a,s')$ and $\\mathcal{R}(s,a)$ are unknown, let's use the algorithm shown below to get an estimate $\\hat{v}_\\pi(s)$ of the true state-value function $v_\\pi(s)$\n",
    "\n",
    "![MCPolicyEvaluation.png](https://i.postimg.cc/6pXj5P6D/MCPolicy-Evaluation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a64d6f-4efa-4906-b18f-9a2b06a4c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_first_visit_policy_evaluation(grid_env, episodes):\n",
    "    \"\"\"\n",
    "    Compute estimate of state-value function for a given policy\n",
    "    :param grid_env (GridEnv): Environment\n",
    "    :param episodes (List(episode)): Number of episodes to use for prediction\n",
    "    \n",
    "    :return List(float): Prediction error after each episode\n",
    "    :return dict(float): Predicted state-value function\n",
    "\n",
    "    \"\"\"\n",
    "    all_states = grid_env.get_states()\n",
    "\n",
    "    # Counter of visits for all states\n",
    "    state_visits = {s:0 for s in all_states}\n",
    "    # Cummulative return for each state\n",
    "    state_returns = {s:0 for s in all_states}\n",
    "    # Predicted state-value function\n",
    "    pred_v = {s:0 for s in all_states}\n",
    "\n",
    "    for episode in episodes:\n",
    "        # Create auxiliary variable to keep of first state visits\n",
    "        visited = {s: False for s in all_states}\n",
    "        # Return for current episode\n",
    "        g = 0\n",
    "\n",
    "        # Starting from last sampled observation in episode\n",
    "        for obs in episode[::-1]:\n",
    "            # Get visited state\n",
    "            s = obs.state\n",
    "            #TODO: Compute return for this state ----------------\n",
    "            g = \n",
    "            #ENDTODO --------------------------------------------\n",
    "            if not visited[s]:\n",
    "                #TODO: Update state_visits, state_returns, pred_v, and visited ------\n",
    "                state_visits[s] # Increment the visit counter\n",
    "                state_returns[s] # Add return\n",
    "                pred_v[s] # Compute mean return\n",
    "                visited[s] # Set the state as visited\n",
    "                #ENDTODO ------------------------------------------------------------\n",
    "                \n",
    "    return pred_v, state_visits, state_returns, visited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55095662-d800-4e91-9012-9c9b5fbc4417",
   "metadata": {},
   "source": [
    "### Implement **TD(0) prediction**  \n",
    "\n",
    "**Temporal Difference Method**\n",
    "\n",
    "Estimate $\\hat{v}_\\pi(s)$ of the true state-value function $v_\\pi(s)$\n",
    "\n",
    "![TDPolicyEvaluation.png](https://i.postimg.cc/c4yywX4c/TDPolicy-Evaluation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198341c4-ff64-4788-99d8-ebe115a70b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_learning_policy_evaluation(grid_env, episodes, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Compute estimate of state-value function for a given policy\n",
    "    :param grid_env (GridEnv): Environment\n",
    "    :param episodes (List(episode)): Number of episodes to use for prediction\n",
    "    :param alpha (float): step-size\n",
    "\n",
    "    :return List(float): Prediction error after each episode\n",
    "    :return dict(float): Predicted state-value function\n",
    "\n",
    "    \"\"\"\n",
    "    all_states = grid_env.get_states()\n",
    "    # Predicted state-value function\n",
    "    pred_v = {s:0 for s in all_states}\n",
    "\n",
    "    for episode in episodes:\n",
    "        # Starting from the first sampled observation in episode\n",
    "        # remember that you have access to: \n",
    "        #   - obs.reward, \n",
    "        #   - obs.next_state, \n",
    "        #   - obs.state,\n",
    "        for obs in episode:\n",
    "            # Get visited state\n",
    "            s = obs.state\n",
    "            #TODO: Update the v(s) --------------------------------------\n",
    "            pred_v[s]\n",
    "            #ENDTODO ----------------------------------------------------\n",
    "    return pred_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2baa5-98b7-4a49-884f-529f2b0f9ec8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import otter\n",
    "grader = otter.Notebook(tests_dir = \"Practical11_Support/tests\")\n",
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcac375f-0c09-44ee-9dcf-08b6a557b6bc",
   "metadata": {},
   "source": [
    "## 4) Visualization & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f5910-107f-4c2b-aff9-2893a67412e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_env = GridEnv(gamma=0.99, noise=0.2, living_reward=-0.04)\n",
    "n_episodes=1000\n",
    "episodes = []\n",
    "for i in range(n_episodes):\n",
    "    episode = generate_episode(grid_env, pi_random)\n",
    "    episodes.append(episode)\n",
    "\n",
    "V_mc, _, _, _ = monte_carlo_first_visit_policy_evaluation(grid_env, episodes)\n",
    "V_td = temporal_learning_policy_evaluation(grid_env, episodes, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a2cbf-4fdd-4f13-a4b2-d1079c8a7858",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_env = GridEnv(gamma=0.99, noise=0.2, living_reward=-0.04)\n",
    "print(\"MC Value Function\")\n",
    "plot_value_function(grid_env, V_mc)\n",
    "plt.show()\n",
    "print(\"TD(0) Value Function\")\n",
    "plot_value_function(grid_env, V_td)\n",
    "plt.show()\n",
    "print(\"One of optimal policy\")\n",
    "plot_policy(grid_env, np.array([[ 3,  3,  3, -1], [ 0, -1,  0, -1], [ 0,  3,  0,  2]]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
